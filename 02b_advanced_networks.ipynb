{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music machine learning - Advanced neural networks\n",
    "\n",
    "### Author: Philippe Esling (esling@ircam.fr)\n",
    "\n",
    "In this course we will cover\n",
    "1. An introduction to [convolutions](#convolution) and how they can be used\n",
    "2. Defining a [Convolutional Neural Network](#cnn) in Pytorch for image classification \n",
    "3. Coding our own [convolutional layer](#layer)\n",
    "4. An explanation on [recurrent networks](#rnn) in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"convolution\"></a>\n",
    "## Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In purely mathematical terms, convolution is a function derived from two given functions by integration which expresses how the shape of one is modified by the other. In simpler (discrete) terms, the convolution product of a matrix by a smaller one can be seen as _filtering_ the large matrix. Hence we slide the small matrix over the large one and compute local products at each position. Therefore the convolution operator $\\star$ computes at each position $n$\n",
    "\n",
    "$$\n",
    "(f \\star g)[n]=\\sum _{m=-M}^{M}f[n-m]g[m].\n",
    "$$\n",
    "\n",
    "An example of this operation is shown here\n",
    "\n",
    "<img src=\"images/02_convolution.png\" align=\"center\"/>\n",
    "\n",
    "This operation can be used to _filter_ the image (as in the _gaussian blur_ operator), or _detect_ features (such as edges). \n",
    "\n",
    "Given an 32x32 image with RGB channels, we can represent it as a tensor of shape `(32, 32, 3)` which is (height, width, channels). When we perform convolution, we need a filter that has the same channel depth as the image. For example, we can use a 5x5 filter which is of shape `(5, 5, 3)` and slide it across the image left to right, top to bottom with a stride of 1 to perform convolution. We are going to perform this in numpy, depending on a certain amount of parameters, which define the behavior of our convolution\n",
    "\n",
    "* `height` and `width`: spatial extend of the filters\n",
    "* `S`: stride size (number of steps to jump to the next position)\n",
    "* `P`: amount of padding (adding zeros in the original matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Padding\n",
    "pad = 2\n",
    "stride = 1\n",
    "height, width = 5, 5\n",
    "# A random fake image\n",
    "x = np.random.randn(3, 32, 32)\n",
    "x_h, x_w = x.shape[1:]\n",
    "# Our convolution kernel\n",
    "weight = np.random.randn(3, 5, 5)\n",
    "# Padding the original image\n",
    "x_pad = np.pad(x, pad_width=((0, 0,), (pad, pad), (pad, pad)), mode='constant', constant_values=0)\n",
    "# We can expect the output size to be\n",
    "h_out = int(1 + (x_h + 2 * pad - height) / stride)\n",
    "w_out = int(1 + (x_w + 2 * pad - width) / stride)\n",
    "# So we will store our result in\n",
    "y = np.zeros((1, h_out, w_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the convolution itself can be performed by using the following loop (which amounts to _slide_ our kernel across the large matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sliding the kernel across the matrix\n",
    "for h in range(h_out):\n",
    "    for w in range(w_out):\n",
    "        i, j = h * stride, w * stride\n",
    "        conv_sum = np.sum(x_pad[:, i:i+height, j:j+width] * weight)\n",
    "        y[0, h, w] = conv_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can witness the effect of this operation with the following plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(13,6))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "img1 = ax1.imshow(x.T,\n",
    "    aspect=\"auto\" ,\n",
    "    )\n",
    "\n",
    "#ax1.legend()\n",
    "ax1.set_xlabel(\"\")\n",
    "ax1.set_ylabel(r\"\")\n",
    "ax1.set_title(r\"Original image\")\n",
    "\n",
    "img2 = ax2.imshow(y.T,\n",
    "    aspect=\"auto\" ,\n",
    "    cmap='gray'\n",
    "    )\n",
    "\n",
    "#ax2.legend()\n",
    "ax2.set_xlabel(\"\")\n",
    "ax2.set_ylabel(r\"\")\n",
    "ax2.set_title(r\"Filter with convolution\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNN)\n",
    "\n",
    "Convolutional NNs (CNNs) rely on convolution in place of general matrix multiplication. They are specialized for processing data with a known grid-like topology and are among the best performing systems in classification/recognition tasks. Each layer in a CNN consists in a set of $N$ _filters_ called _kernels_, that are convolved across the input. If we denote as $\\{k^l_n\\}_{n\\in[1;N]}$ the set of kernels for layer $l$, these all share a unique _kernel size_. By convolving each one of its $N$ kernels across a d-dimensional input $x$, a convolutional layer produces $N$ d-dimensional outputs called _feature maps_, denoted as $\\{a^l_n\\}_{n\\in[1;N]}$. Hence, the computation of the $n$-th activation map in layer $l$ for input $x$ is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "a^l_n = \\sum_{m=1}^{M} k^l_n \\star x_m + b^l_n\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Thus, as depicted in the following Figure, the feature map corresponding to kernel $n$ consists in the sum of the d-dimensional discrete convolutions (denoted by the $\\star$ operator) between the kernel $n$ and each one of the d-dimensional data $\\{x_m\\}_{m\\in[1;M]}$, plus a bias $b$. A convolutional layer is thus a 3-dimensional tensor $h \\in \\mathcal{T}_{N,I,J}(\\mathbb{R})$ where $N$ is the number of features maps while $I$ and $J$ are respectively the _width_ and _height_ of the maps. \n",
    "\n",
    "<img src=\"images/02_cnns.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be seen as replacing our _neurons_ by _feature detectors_ (the convolutional kernels), which will increasingly process the image. In the following, we will first use the high-level interface of `Pytorch` to define a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cnn\"></a>\n",
    "## Defining a CNN in Pytorch : OBLIGATOIRE\n",
    "\n",
    "Defining a convolutional network in Pytorch is quite easy, as we can rely on the `nn` module, which contains all the required layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous course, we have seen that we could define our network in a very simple way, by using the `Sequential` model definition. Here we define a CNN followed by a MLP, as seen in the previous course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input dimensions\n",
    "in_size = 1000\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 6, 5), #1st layer\n",
    "    nn.ReLU(),          #2nd layer\n",
    "    nn.MaxPool2d(2, 2), #...\n",
    "    nn.Conv2d(6, 16, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 5 * 5, 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(84, 10),\n",
    "    nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to implement the networks in Pytorch is to use the `functional` approach. In this version, each layer is seen as a function, that we apply on sucessive inputs. For instance, we can define one layer of fully-connected units and apply it to some inputs as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# Define one layer\n",
    "layer = nn.Linear(100, 10)\n",
    "# Define the non-linearity\n",
    "activation = nn.ReLU()\n",
    "# Create some random input\n",
    "inputs = torch.rand(32, 100)\n",
    "# Apply our layers\n",
    "outputs = activation(layer(inputs))\n",
    "# Equivalently, as ReLU is parameter-free\n",
    "outputs = F.relu(layer(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to make this even cleaner, we can define our own `nn.Module`, which is a `Pytorch` class representing models. To do so, we can define a sub-class, and implement the functions `__init__` (defining our layers) and `forward` (explaining how our forward pass will behave)\n",
    "\n",
    "***\n",
    "\n",
    "**Exercise**\n",
    "1. Implement a CNN in Pytorch using the `functional` library : OBLIGATOIRE\n",
    "\n",
    "***\n",
    "\n",
    "<!--\n",
    "<div class=\"alert alert-info\" markdown=1><h4>Exercise</h4>\n",
    "1. Implement a CNN in Pytorch using the `functional` library\n",
    "</div>\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.linear2 = nn.Linear(120, 84)\n",
    "        self.linear3 = nn.Linear(84, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        l1 = F.relu(self.conv1(x))\n",
    "        l2 = self.pool1(l1)\n",
    "        l3 = F.relu(self.conv2(l2))\n",
    "        l4 = self.pool2(l3)\n",
    "        l4 = self.flatten(l4)\n",
    "        l5 = F.relu(self.linear1(l4))\n",
    "        l6 = F.relu(self.linear2(l5))\n",
    "        l7 = self.linear3(l6)\n",
    "        o = F.softmax(l7,dim=1)\n",
    "        return o\n",
    "\n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network : OBLIGATOIRE\n",
    "\n",
    "In order to test our CNN, we are going to try to perform image classification. To do so, we can use the simplifications for data loading contained in `torchvision`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torchvision` package provides pre-coded simplification to download and use the major image datasets, notably `MNIST` and `CIFAR`, which are the baseline datasets for testing image ML models. The output of torchvision datasets are PILImage images of range [0, 1]. We transform them to Tensors of normalized range [-1, 1].\n",
    "\n",
    "In the following code, we are going to load the `CIFAR10` _train_ and _test_ sets. **Note that this code will automatically download the dataset if you did not have it before, and place it in the `data` folder, so this might take a bit of time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Transforms to apply to the images\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# Import the train dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=True, num_workers=2)\n",
    "# Import the test dataset\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=False, num_workers=2)\n",
    "# Classes in the CIFAR dataset\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using your knowledge from the previous course, you can now define an optimization problem, and implement the training loop for your model\n",
    "\n",
    "***\n",
    "\n",
    "**Exercise**\n",
    "1. Define a `criterion` and `optimizer` : OBLIGATOIRE\n",
    "2. Fill in the training loop to train your model : OBLIGATOIRE\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()       #Une fonction loss particulière, adaptée mais je sais pas pq\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.001,momentum=0.9)     #essayer SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss at epoch 0: 2.302113626909256\n",
      "mean loss at epoch 1: 2.262220694541931\n",
      "mean loss at epoch 2: 2.167121898794174\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#forward\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_pred,labels)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#backward\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m l1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x))\n\u001b[1;32m     19\u001b[0m l2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(l1)\n\u001b[0;32m---> 20\u001b[0m l3 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml2\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     21\u001b[0m l4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool2(l3)\n\u001b[1;32m     22\u001b[0m l4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(l4)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    # Go through all batches\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        #forward\n",
    "        y_pred = model(inputs)\n",
    "        \n",
    "        loss = criterion(y_pred,labels)\n",
    "\n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Keep track of the loss\n",
    "        running_loss += loss.item()\n",
    "    print(f'mean loss at epoch {epoch}: {running_loss/len(trainloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your model is trained, you can test it by feeding some new (unseen) images and see if it is able to classify them correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"layer\"></a>\n",
    "## Coding our own convolutional layer\n",
    "\n",
    "Although `Pytorch` comes packed with pre-implemented layers, we can also very easily define our own layers. This will be useful when you start doing research and propose your own way of processing the information. A large advantage of `Pytorch` is that it performs _automatic gradient differentiation_, this means that we simply have to define how the `forward` pass will work, and `Pytorch` will automatically infer the backpropagation equations, without us having to go through any complicated differentiation\n",
    "\n",
    "In the following, we are going to redefine the `Conv2d` layer, by computing the operation ourselves.\n",
    "\n",
    "***\n",
    "\n",
    "**Exercise**\n",
    "1. Complete the `forward` function to compute a convolution\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConv2d(nn.Module):\n",
    "    def __init__(self, n_channels, out_channels, kernel_size, dilation=1, padding=0, stride=1):\n",
    "        super(MyConv2d, self).__init__()\n",
    "        self.kernel_size = (kernel_size, kernel_size)\n",
    "        self.kernal_size_number = kernel_size * kernel_size\n",
    "        self.out_channels = out_channels\n",
    "        self.dilation = (dilation, dilation)\n",
    "        self.padding = (padding, padding)\n",
    "        self.stride = (stride, stride)\n",
    "        self.n_channels = n_channels\n",
    "        self.weights = nn.Parameter(torch.Tensor(self.out_channels, self.n_channels, self.kernal_size_number))\n",
    "\n",
    "    def forward(self, x):\n",
    "        width = self.calculateNewWidth(x)\n",
    "        height = self.calculateNewHeight(x)\n",
    "        windows = self.calculateWindows(x)\n",
    "        \n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE\n",
    "        ######################\n",
    "        \n",
    "        return result  \n",
    "\n",
    "    def calculateWindows(self, x):\n",
    "        windows = F.unfold(x, kernel_size=self.kernel_size, padding=self.padding, dilation=self.dilation, stride=self.stride)\n",
    "        windows = windows.transpose(1, 2).contiguous().view(-1, x.shape[1], self.kernal_size_number)\n",
    "        windows = windows.transpose(0, 1)\n",
    "        return windows\n",
    "\n",
    "    def calculateNewWidth(self, x):\n",
    "        return ((x.shape[2] + 2 * self.padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1) // self.stride[0]) + 1\n",
    "\n",
    "    def calculateNewHeight(self, x):\n",
    "        return ((x.shape[3] + 2 * self.padding[1] - self.dilation[1] * (self.kernel_size[1] - 1) - 1)// self.stride[1]) + 1\n",
    "\n",
    "# Testing the code directly\n",
    "conv = MyConv2d(3, 1, 3)\n",
    "x = torch.randn(1, 3, 24, 24)\n",
    "out = conv(x)\n",
    "out.mean().backward()\n",
    "# Check that we do have gradients\n",
    "print(conv.weights.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use your own `MyConv2d` layer, and use it in real-life scenarios, by trying to change your previous model to use your own layer instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks (RNNs) are a family of models designed to process time series and sequential data, which perform remarkably in applications such as speech recognition or machine translation. The ability of RNNs to model correlations between successive computations through recurrent connection make them efficient for temporal problem as they provide a form of _memory_. \n",
    "\n",
    "To model structured sequential data, NNs can be augmented with recurrent loops, which allow to retain information across time steps. Considering a sequence $\\mathbf{X}=\\{\\mathbf{x}_t\\}$, dependencies between elements are managed by having a recurrent hidden state $\\mathbf{h}_t$ at time $t$ in the network. The value of $\\mathbf{h}_t$ at each time depends of the previous time and the input, as depicted in the following figure. \n",
    "\n",
    "<img src=\"images/02_rnn.png\" align=\"center\"/>\n",
    "\n",
    "Formally, each hidden state is updated as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{h}_t = \n",
    "\\begin{cases} \n",
    "\\phi_{\\mathbf{\\mathbf{\\theta}}}(\\mathbf{x}_0) & \\mbox{if } t=0 \\\\ \n",
    "\\phi_{\\mathbf{\\mathbf{\\theta}}}(\\mathbf{h}_{t-1},\\mathbf{x}_t), & \\mbox{otherwise} \n",
    "\\end{cases}\n",
    "\\label{eq:RNNhiddenupdate}\n",
    "\\end{equation}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a simple rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple one-to-many vanilla recurrent neural network example in functional form. If we were to produce `h[t]`, we need some weight matrices, `h[t-1]`, `x[t]` and a non-linearity `tanh`.\n",
    "\n",
    "$$\n",
    "h_{t} = tanh(W_{hh}h_{t-1} + W_{xh}x_{t} + B_{h})\n",
    "$$\n",
    "\n",
    "Since this is a **one-to-many** network, we'd want to produce an output `y[t]` at every timestep, thus, we need another weight matrix that accepts a hidden state and project it to an output.\n",
    "\n",
    "$$\n",
    "y_{t} = W_{hy}h_{t} + B_{y}\n",
    "$$\n",
    "\n",
    "Now that we know how to use the `Functional` library of `Pytorch`, we are going to implement our own simple RNN layer as previously. This time, we do not provide the content of the `__init__` function, so think carefully of what parameters you will need and how you need to define them.\n",
    "\n",
    "***\n",
    "\n",
    "**Exercise**\n",
    "1. Complete the `__init__` function to instantiate all required parameters\n",
    "1. Complete the `forward` function to compute the forward pass\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RecurrentNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\" Defines the required variables \"\"\"\n",
    "        super(RecurrentNetwork, self).__init__()\n",
    "        \n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE\n",
    "        ######################\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Computes the forward pass \"\"\"\n",
    "        \n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE\n",
    "        ######################\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then forward propagate our information inside our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = 4\n",
    "input_vector = torch.ones(16, in_size)\n",
    "simple_network = RecurrentNetwork(in_size, 10, 5)\n",
    "# Notice that same input, but leads to different ouptut at every single time step.\n",
    "print(simple_network(input_vector))\n",
    "print(simple_network(input_vector))\n",
    "print(simple_network(input_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, `Pytorch` also comes packed with some pre-coded recurrent layers. You can go check the documentation to find how to use these."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
